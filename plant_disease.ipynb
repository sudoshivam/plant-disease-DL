{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PaDzDcvsnH77"
   },
   "outputs": [],
   "source": [
    "# Note: colab has changed the tensorflow version from 27 March 2020\n",
    "# %tensorflow_version 1.x\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "# from keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "img_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "xP7tSCPLnWk3"
   },
   "outputs": [],
   "source": [
    "# load train and test dataset\n",
    "\n",
    "\n",
    "# classes: 'Yellow_Leaf_Curl_Virus', 'Target_Spot', 'Spider_mites', \n",
    "#          'Septoria_leaf_spot', 'Mosaic_virus', 'Leaf_Mold', 'Late_blight', \n",
    "#          'Bacterial_spot', 'Early_blight', 'Healthy'\n",
    "\n",
    "def prepare_data(path, img_size):\n",
    "    X = []\n",
    "    Y = []\n",
    "    filenames = [img for img in glob.glob(path)]\n",
    "    for i in tqdm(filenames):\n",
    "        img = cv2.imread(i)\n",
    "        img = cv2.resize(img, dsize=(img_size, img_size))\n",
    "        X.append(img)\n",
    "        split = i.split(os.sep)[-2]\n",
    "        if split == \"Tomato___Tomato_Yellow_Leaf_Curl_Virus\":\n",
    "            Y.append(0)\n",
    "        elif split == \"Tomato___Target_Spot\":\n",
    "            Y.append(1)\n",
    "        elif split == \"Tomato___Spider_mites Two-spotted_spider_mite\":\n",
    "            Y.append(2)\n",
    "        elif split == \"Tomato___Septoria_leaf_spot\":\n",
    "            Y.append(3)\n",
    "        elif split == \"Tomato___Tomato_mosaic_virus\":\n",
    "            Y.append(4)\n",
    "        elif split == \"Tomato___Leaf_Mold\":\n",
    "            Y.append(5)\n",
    "        elif split == \"Tomato___Late_blight\":\n",
    "            Y.append(6)\n",
    "        elif split == \"Tomato___Bacterial_spot\":\n",
    "            Y.append(7)\n",
    "        elif split == \"Tomato___Early_blight\":\n",
    "            Y.append(8)\n",
    "        elif split == \"Tomato___healthy\":\n",
    "            Y.append(9)  \n",
    "    X = np.asarray(X)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmOqS5WkF70x",
    "outputId": "3a73fe46-c9ba-4bf4-d855-efab7ed1c9bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 515.47it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = prepare_data(r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\dataset\\train\\*\\*', img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZHPH6gIJ68-",
    "outputId": "a9c162ab-cc61-46a4-b07d-95224f888b6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 235.79it/s]\n"
     ]
    }
   ],
   "source": [
    "x_val, y_val = prepare_data(r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\dataset\\val\\*\\*', img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDqOWL_uBlxv",
    "outputId": "ab726524-9a43-49b0-862c-21c5038333fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train in MBs: 187.5\n",
      "x_val in MBs: 37.5\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train in MBs:\", x_train.nbytes/(1024*1024))\n",
    "print(\"x_val in MBs:\", x_val.nbytes/(1024*1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "-cr5iDfLGlh7"
   },
   "outputs": [],
   "source": [
    "on_save = 1\n",
    "on_load = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "g4B_2bq8FDfz"
   },
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "\n",
    "if on_save == 1:\n",
    "  save(r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\saved_arrays\\tomato\\trainX_'+str(img_size)+'.npy', x_train)\n",
    "  save(r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\saved_arrays\\tomato\\trainY_'+str(img_size)+'.npy', y_train)\n",
    "  save(r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\saved_arrays\\tomato\\valX_'+str(img_size)+'.npy', x_val)\n",
    "  save(r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\saved_arrays\\tomato\\valY_'+str(img_size)+'.npy', y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "dlUmAsOPFGv8"
   },
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "\n",
    "if on_load == 1:\n",
    "  train_x = load(r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\saved_arrays\\tomato\\trainX_'+str(img_size)+'.npy')\n",
    "  train_y = load(r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\saved_arrays\\tomato\\trainY_'+str(img_size)+'.npy')\n",
    "  val_x   = load(r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\saved_arrays\\tomato\\valX_'+str(img_size)+'.npy')\n",
    "  val_y   = load(r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\saved_arrays\\tomato\\valY_'+str(img_size)+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "aJhwa9qgnOhv"
   },
   "outputs": [],
   "source": [
    "# Scale up the pixels\n",
    "\n",
    "def prep_pixels(trainX, valX):\n",
    "\ttrain_norm = trainX.astype('float32')\n",
    "\tval_norm = valX.astype('float32')\n",
    "\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\tval_norm = val_norm / 255.0\n",
    "\n",
    "\treturn train_norm, val_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "UMQhN-pTGOsL"
   },
   "outputs": [],
   "source": [
    "trainX, valX = prep_pixels(train_x, val_x)\n",
    "\n",
    "# Hot Encoding\n",
    "trainY = to_categorical(train_y)\n",
    "valY = to_categorical(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kc1KK30fOTcd",
    "outputId": "e094acd1-9975-4045-bcb1-b7aeb48b89ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 256, 256, 3)\n",
      "(1000, 10)\n",
      "(200, 256, 256, 3)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "\n",
    "print(valX.shape)\n",
    "print(valY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYEk2GDYEBf8",
    "outputId": "06d7c29a-2d53-4d26-a386-a38697740250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory size of a NumPy array in GBs: 0.732421875\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory size of a NumPy array in GBs:\", trainX.nbytes/(1024*1024*1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1O9za1q51gY",
    "outputId": "aca732c9-e589-4851-f6a9-1f30e33af5ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16777344  \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 17,067,946\n",
      "Trainable params: 17,066,794\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shivam\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\tkernel = (3, 3)\n",
    "\tfirst_conv  = 32\n",
    "\tsecond_conv = first_conv * 2\n",
    "\tthird_conv  = second_conv * 2\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(first_conv, kernel, activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(img_size, img_size, 3)))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\t\n",
    "\tmodel.add(Conv2D(first_conv, kernel, activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\t\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Conv2D(second_conv, kernel, activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\t\n",
    "\tmodel.add(Conv2D(second_conv, kernel, activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\t\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(Conv2D(third_conv, kernel, activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\t\n",
    "\tmodel.add(Conv2D(third_conv, kernel, activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\t\n",
    "\tmodel.add(Dropout(0.4))\n",
    "\tmodel.add(Flatten())\n",
    " \n",
    "\tmodel.add(Dense(third_conv, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(BatchNormalization())\n",
    " \n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    " \n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\tmodel.summary()\n",
    "\treturn model\n",
    "\n",
    "# define model\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-zHFKKm8Cvjb"
   },
   "outputs": [],
   "source": [
    "# datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "# it_train = datagen.flow(trainX, trainY, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathModelSave = r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\SavedModels\\tomato_'+str(img_size)+'.hdf5'\n",
    "pathToSaveCSV = r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\SavedModels\\csv\\tomato_'+str(img_size)+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35RB7i6r6WU8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(pathModelSave, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "# early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "csv_logger = CSVLogger(pathToSaveCSV, append=False, separator=',')\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "steps = int(trainX.shape[0] / 32)\n",
    "history = model.fit(\n",
    "          trainX, trainY,\n",
    "          batch_size= 16, \n",
    "          epochs= 30,\n",
    "          callbacks = [checkpoint, csv_logger],\n",
    "          validation_data = (valX, valY),\n",
    "          use_multiprocessing = True,\n",
    "          verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "\n",
    "print(\"Total training time is:\", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tdm_wsdQcMV",
    "outputId": "b7c0aa00-bb00-4cf7-af9c-bbdef7e0bf34"
   },
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "_, acc = model.evaluate(valX, valY, verbose=0)\n",
    "print('without model loading, validation acc is> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhoMnDK0T0au"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "print(\"gpu_options:\", gpu_options)\n",
    "print(\"session:\", session)\n",
    "\n",
    "new_model = load_model(pathModelSave)\n",
    "_, acc_train = new_model.evaluate(trainX, trainY, verbose=1)\n",
    "print('training acc is> %.3f' % (acc_train * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDvHlWfmqIlk"
   },
   "outputs": [],
   "source": [
    "_val, acc_val = new_model.evaluate(valX, valY, verbose=0)\n",
    "print('validation acc is> %.3f' % (acc_val * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w-a1BsJ7ksM"
   },
   "outputs": [],
   "source": [
    "# it is used in below intermediate visualization\n",
    "\n",
    "path_healthy = r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\dataset\\train\\Tomato___Late_blight\\973b8b82-cccd-433c-a868-5dee559b2dce___GHLB Leaf 2.6 Day 12.JPG'\n",
    "img_vis = cv2.imread(path_healthy)\n",
    "img_vis = cv2.resize(img_vis, dsize=(img_size, img_size))\n",
    "img_vis = cv2.cvtColor(img_vis, cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(cv2.cvtColor(img_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.imshow(img_vis)\n",
    "print(img_vis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Z-1llECRrRye"
   },
   "outputs": [],
   "source": [
    "model = load_model(pathModelSave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WftXe6oRegqJ"
   },
   "outputs": [],
   "source": [
    "# It is for intermediate conv2d layers visualization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = Model(inputs = model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(img_vis.reshape(1,img_size,img_size,3))\n",
    "\n",
    "def display_activation(activations, col_size, row_size, act_index): \n",
    "    activation = activations[act_index]\n",
    "    activation_index = 0\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*4.5, col_size*2.5)) # sharex=True, sharey=True\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].imshow(activation[0][:, :, activation_index], cmap='PuOr')\n",
    "            ax[row,col].axis('off')\n",
    "            activation_index += 1\n",
    "\n",
    "display_activation(activations, 5, 5, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "yMo4iuJT7pCd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def convert_string_label(intLabel):\n",
    "  Y = []\n",
    "  length = len(intLabel)\n",
    "    \n",
    "  for i in range(length):\n",
    "    if intLabel[i] == 0:\n",
    "      Y.append(\"Yellow_Leaf_Curl_Virus\")\n",
    "    elif intLabel[i] == 1:\n",
    "      Y.append(\"Target_Spot\")\n",
    "    elif intLabel[i] == 2:\n",
    "      Y.append(\"Spider_mites\")\n",
    "    elif intLabel[i] == 3:\n",
    "      Y.append(\"Septoria_leaf_spot\")\n",
    "    elif intLabel[i] == 4:\n",
    "      Y.append(\"Mosaic_virus\")\n",
    "    elif intLabel[i] == 5:\n",
    "      Y.append(\"Leaf_Mold\")\n",
    "    elif intLabel[i] == 6:\n",
    "      Y.append(\"Late_blight\")\n",
    "    elif intLabel[i] == 7:\n",
    "      Y.append(\"Bacterial_spot\")\n",
    "    elif intLabel[i] == 8:\n",
    "      Y.append(\"Early_blight\")\n",
    "    elif intLabel[i] == 9:\n",
    "      Y.append(\"Healthy\")    \n",
    "  return Y\n",
    "  \n",
    "    \n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        \n",
    "    figure(num=None, figsize=(8, 6), dpi=120, facecolor='w', edgecolor='red')\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlhbcurdSFJT"
   },
   "outputs": [],
   "source": [
    "model = load_model(pathModelSave)\n",
    "\n",
    "y_pred = model.predict(valX)\n",
    "y_pred = np.rint(y_pred.argmax(axis=1))\n",
    "y_pred = y_pred.tolist()\n",
    "\n",
    "xyz = [np.argmax(y, axis=None, out=None) for y in val_y]\n",
    "valY = np.rint(xyz)\n",
    "valY = valY.tolist()\n",
    "\n",
    "y_true      = convert_string_label(valY)\n",
    "y_predicted = convert_string_label(y_pred)\n",
    "\n",
    "# print(\"y_true:\", y_true, \"\\n\\n\")\n",
    "# print(\"y_predicted:\", y_predicted)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_true, y_predicted, labels=['Yellow_Leaf_Curl_Virus', 'Target_Spot', 'Spider_mites', 'Septoria_leaf_spot', 'Mosaic_virus', 'Leaf_Mold', 'Late_blight', 'Bacterial_spot', 'Early_blight', 'Healthy'])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Yellow_Leaf_Curl_Virus', 'Target_Spot', 'Spider_mites', 'Septoria_leaf_spot', 'Mosaic_virus', 'Leaf_Mold', 'Late_blight', 'Bacterial_spot', 'Early_blight', 'Healthy'], title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "sFLAkzCnRgJy"
   },
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "\t\n",
    "  # plot loss\n",
    "  plt.subplot(211)\n",
    "  plt.title('Cross Entropy Loss')\n",
    "  plt.plot(history.history['loss'], color='blue', label='train')\n",
    "  plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "  plt.show()\n",
    "\n",
    "  # plot accuracy\n",
    "  plt.subplot(212)\n",
    "  plt.title('Classification Accuracy')\n",
    "  plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "  plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "  plt.show()\n",
    "  \n",
    "  # save plot to file\n",
    "  # filename = sys.argv[0].split('/')[-1]\n",
    "  # pyplot.savefig(filename + '_plot.png')\n",
    "  # pyplot.close()\n",
    "\n",
    "  plt.plot(history.history[\"accuracy\"])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title(\"model accuracy\")\n",
    "\n",
    "  plt.ylabel(\"Accuracy\")\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.legend([\"Training Accuracy\",\"Validation Accuracy\",\"Training loss\",\"Validation Loss\"])\n",
    "  plt.show()\n",
    "\n",
    "  # Plot training & validation accuracy values\n",
    "  plt.plot(history.history['accuracy'])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.title('Model accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Training Accuracy', 'Validation Accuracy'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Training Loss', 'Validation Loss'], loc='upper left')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGWttQxWZ2Gu"
   },
   "outputs": [],
   "source": [
    "# learning curves\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "BKwURArK1t40"
   },
   "outputs": [],
   "source": [
    "# this is the new prediction function <optimized> to calcultae prediction time per image\n",
    "model = load_model(pathModelSave)\n",
    "\n",
    "def new_prediction_fun(path):\n",
    "  filenames = [img for img in glob.glob(path + '/*')]\n",
    "  count = 0\n",
    "  total_time = 0\n",
    "  YLCV, TS, SM, SLS, MV, LM, LB, BS, EB, H = 0,0,0,0,0,0,0,0,0,0\n",
    "  for i in filenames:\n",
    "      count += 1\n",
    "      tic = time.clock()\n",
    "      img = cv2.imread(i)\n",
    "      img = cv2.resize(img, dsize=(img_size, img_size))\n",
    "      img = img.astype('float32')\n",
    "      img = img / 255.0\n",
    "      img = img[np.newaxis, :]\n",
    "      p = model.predict(img)[0]\n",
    "      max_val_index = p.argmax()\n",
    "      toc = time.clock()\n",
    "      time_t = toc-tic\n",
    "      \n",
    "      if (max_val_index == 0):\n",
    "        YLCV += 1\n",
    "      elif (max_val_index == 1):\n",
    "        TS += 1\n",
    "      elif (max_val_index == 2):\n",
    "        SM += 1\n",
    "      elif (max_val_index == 3):\n",
    "        SLS += 1\n",
    "      elif (max_val_index == 4):\n",
    "        MV += 1\n",
    "      elif (max_val_index == 5):\n",
    "        LM += 1\n",
    "      elif (max_val_index == 6):\n",
    "        LB += 1\n",
    "      elif (max_val_index == 7):\n",
    "        BS += 1\n",
    "      elif (max_val_index == 8):\n",
    "        EB += 1\n",
    "      elif (max_val_index == 9):\n",
    "        H += 1  \n",
    "      \n",
    "    \n",
    "      total_time += time_t\n",
    "  \n",
    "  print(\"\\n\")    \n",
    "  print(\"Total prediction time for \"+str(count)+\" images is: \", total_time, \"sec\")\n",
    "  print(\"Average prediction time is:\", total_time/count, \"sec\")\n",
    "  \n",
    "  print(\"\\n\")\n",
    "  print(\"YLCV predictions out of total \"+str(count)+\" \"+str(os.path.split(path)[1])+\" images are: \", YLCV)\n",
    "  print(\"TS predictions out of total \"+str(count)+\" \"+str(os.path.split(path)[1])+\" images are: \", TS)\n",
    "  print(\"SM predictions out of total \"+str(count)+\" \"+str(os.path.split(path)[1])+\" images are: \", SM)\n",
    "  print(\"SLS predictions out of total \"+str(count)+\" \"+str(os.path.split(path)[1])+\" images are: \", SLS)\n",
    "  print(\"MV predictions out of total \"+str(count)+\" \"+str(os.path.split(path)[1])+\" images are: \", MV)\n",
    "  print(\"LM predictions out of total \"+str(count)+\" \"+str(os.path.split(path)[1])+\" images are: \", LM)\n",
    "  print(\"LB predictions out of total \"+str(count)+\" \"+str(os.path.split(path)[1])+\" images are: \", LB)\n",
    "  print(\"BS predictions out of total \"+str(count)+\" \"+str(os.path.split(path)[1])+\" images are: \", BS)\n",
    "  print(\"EB predictions out of total \"+str(count)+\" \"+str(os.path.split(path)[1])+\" images are: \", EB)\n",
    "  print(\"H predictions out of total \"+str(count)+\" \"+str(os.path.split(path)[1])+\" images are: \", H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgsVjBzlJmnt"
   },
   "outputs": [],
   "source": [
    "# augumented testing, more images\n",
    "new_prediction_fun(r'D:\\Work\\Projects\\Meity\\tomato-disease-prediction\\model01\\dataset\\train\\Tomato___Late_blight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqFS552PvCyj"
   },
   "outputs": [],
   "source": [
    "print(type(history))\n",
    "...\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hn7EJTZnw2Mf"
   },
   "outputs": [],
   "source": [
    "print(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oKRTS_9yG0n"
   },
   "outputs": [],
   "source": [
    "print(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tqyw1_KiyHuM"
   },
   "outputs": [],
   "source": [
    "print(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7p46PMHyIZ6"
   },
   "outputs": [],
   "source": [
    "print(history.history['val_accuracy'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Tomato Problem-local | MMMUT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
